{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx5uTIxlbQLW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from random import sample\n",
        "from skimage.morphology import skeletonize\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Omniglot Dataset"
      ],
      "metadata": {
        "id": "h6ycnsp09jhK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EEEVIODcOWP"
      },
      "outputs": [],
      "source": [
        "size = 105\n",
        "class Dataset:\n",
        "    # This class will facilitate the creation of a few-shot dataset\n",
        "    # from the Omniglot dataset that can be sampled from quickly while also\n",
        "    # allowing to create new labels at the same time.\n",
        "    def __init__(self, training):\n",
        "        # Download the tfrecord files containing the omniglot data and convert to a\n",
        "        # dataset.\n",
        "        split = \"train\" if training else \"test\"\n",
        "        ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n",
        "        # Iterate over the dataset to get each individual image and its class,\n",
        "        # and put that data into a dictionary.\n",
        "        self.data = {}\n",
        "\n",
        "        def extraction(image, label):\n",
        "            # This function will shrink the Omniglot images to the desired size,\n",
        "            # scale pixel values and convert the RGB image to grayscale\n",
        "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "            image = tf.image.rgb_to_grayscale(image)\n",
        "            image = tf.image.resize(image, [size, size])\n",
        "            return image, label\n",
        "\n",
        "        for image, label in ds.map(extraction):\n",
        "            image = image.numpy()\n",
        "            label = str(label.numpy())\n",
        "            if label not in self.data:\n",
        "                self.data[label] = []\n",
        "            self.data[label].append(image)\n",
        "        self.labels = list(self.data.keys())\n",
        "\n",
        "    def get_mini_dataset(\n",
        "        self, batch_size, repetitions, shots, num_classes, split=False\n",
        "    ):\n",
        "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
        "        temp_images = np.zeros(shape=(num_classes * shots, size, size, 1))\n",
        "        if split:\n",
        "            test_labels = np.zeros(shape=(num_classes))\n",
        "            test_images = np.zeros(shape=(num_classes, size, size, 1))\n",
        "\n",
        "        # Get a random subset of labels from the entire label set.\n",
        "        label_subset = random.choices(self.labels, k=num_classes)\n",
        "        for class_idx, class_obj in enumerate(label_subset):\n",
        "            # Use enumerated index value as a temporary label for mini-batch in\n",
        "            # few shot learning.\n",
        "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
        "            # If creating a split dataset for testing, select an extra sample from each\n",
        "            # label to create the test dataset.\n",
        "            if split:\n",
        "                test_labels[class_idx] = class_idx\n",
        "                images_to_split = random.choices(\n",
        "                    self.data[label_subset[class_idx]], k=shots + 1\n",
        "                )\n",
        "                test_images[class_idx] = images_to_split[-1]\n",
        "                temp_images[\n",
        "                    class_idx * shots : (class_idx + 1) * shots\n",
        "                ] = images_to_split[:-1]\n",
        "            else:\n",
        "                # For each index in the randomly selected label_subset, sample the\n",
        "                # necessary number of images.\n",
        "                temp_images[\n",
        "                    class_idx * shots : (class_idx + 1) * shots\n",
        "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
        "        )\n",
        "        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n",
        "        if split:\n",
        "            return dataset, test_images, test_labels\n",
        "        return dataset\n",
        "\n",
        "\n",
        "import urllib3\n",
        "\n",
        "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
        "train_dataset = Dataset(training=True)\n",
        "test_dataset = Dataset(training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organize Alphabets"
      ],
      "metadata": {
        "id": "oRfOssTU9oN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRYGK1bH8sME"
      },
      "outputs": [],
      "source": [
        "import urllib3\n",
        "\n",
        "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
        "train_dataset = Dataset(training=True)\n",
        "test_dataset = Dataset(training=False)\n",
        "\n",
        "ds = tfds.load(\"omniglot\", split=\"train\", as_supervised=False)\n",
        "\n",
        "label_alphabet = {}\n",
        "\n",
        "for i in ds:\n",
        "    alphabet_str = str(i['alphabet'].numpy())\n",
        "    label = i['label'].numpy()\n",
        "\n",
        "    if alphabet_str not in label_alphabet:\n",
        "        label_alphabet[alphabet_str] = []\n",
        "\n",
        "    label_alphabet[alphabet_str].append(label)\n",
        "\n",
        "alphabets = [i for i in label_alphabet.keys()]\n",
        "# print(alphabets)\n",
        "#['27', '30', '17', '12', '15', '37', '43', '48', '32', '3', '2', '21', '25', '13', '14', '35', '26', '20', '0', '38', '4', '16', '41', '24', '11', '10', '31', '5', '45', '22']\n",
        "\n",
        "\n",
        "all_alphabets = []\n",
        "for id in alphabets:\n",
        "  alpha = [str(i) for i in np.unique(label_alphabet[id])]\n",
        "  all_alphabets.append(alpha)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary Functions"
      ],
      "metadata": {
        "id": "--b0bZEO9t1j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtSqbuHxdzzr"
      },
      "outputs": [],
      "source": [
        "def plot(xy, labels):\n",
        "  plt.subplots()\n",
        "  plt.scatter(xy[:, 0], xy[:, 1], c=labels, s=40, cmap='viridis')\n",
        "\n",
        "#use with scale shift\n",
        "def plot_xy(xy):\n",
        "  plt.ylim(-10,35)\n",
        "  plt.xlim(-10,35)\n",
        "  plt.scatter(xy[:, 0], xy[:, 1])\n",
        "\n",
        "#generates scatter samples of the character\n",
        "def generate(xy, n):\n",
        "  gen = mixture.GaussianMixture(n_components= n).fit(xy)\n",
        "  newwxy, newlabels = gen.sample(300) #300 is density of pixels\n",
        "  newxy = newwxy[:,:2]\n",
        "  return newlabels, newxy\n",
        "\n",
        "#outputs two examples of the same class\n",
        "def character(char_class, example1_index, example2_index):\n",
        "  a = train_dataset.data[str(char_class)][example1_index]\n",
        "  a = np.stack((a[:, :, 0],) * 3, axis=2)\n",
        "  a *= 255\n",
        "  a = np.clip(a, 0, 255).astype(\"uint8\")\n",
        "  a = pix2cart(a)\n",
        "  a1 = train_dataset.data[str(char_class)][example2_index]\n",
        "  a1 = np.stack((a1[:, :, 0],) * 3, axis=2)\n",
        "  a1 *= 255\n",
        "  a1 = np.clip(a1, 0, 255).astype(\"uint8\")\n",
        "  a1 = pix2cart(a1)\n",
        "  return a, a1\n",
        "\n",
        "#converts generated characters back into images to feed into the neural network\n",
        "def cart2pix(generated_a):\n",
        "  # Find the maximum value of x and y in the array\n",
        "  max_x, max_y = np.amax(generated_a, axis=0)\n",
        "  # Scale the XY coordinates to fit within the 28x28 pixel grid\n",
        "  scaled_coordinates = np.round(np.array(generated_a) * (28 / max(max_x, max_y))).astype(int)\n",
        "  scaled_coordinates = np.clip(scaled_coordinates, 0, 27)\n",
        "  # Create an empty image with all pixels set to 0\n",
        "  image = np.zeros((28, 28))\n",
        "  # Set the pixel values at the scaled XY coordinates to 1\n",
        "  image[scaled_coordinates[:, 0], scaled_coordinates[:, 1]] = 1\n",
        "  # plt.imshow(image)\n",
        "  return image\n",
        "\n",
        "\n",
        "def oneshot():\n",
        "############### VISUALIZATIONS ######################\n",
        "  _, axarr = plt.subplots(nrows=2, ncols=5, figsize=(10, 6))\n",
        "############### VISUALIZATIONS ######################\n",
        "\n",
        "  sample_keys = list(train_dataset.data.keys())\n",
        "  # Get a random sample of 10 different characters\n",
        "  selected_classes = random.sample(sample_keys, 10)\n",
        "  query_class = random.sample(selected_classes,1)\n",
        "  query = train_dataset.data[query_class[0]][1]\n",
        "  query = np.stack((query[:, :, 0],) * 3, axis=2)\n",
        "  query *= 255\n",
        "  query = np.clip(query, 0, 255).astype(\"uint8\")\n",
        "  comparisons = [] #the other example candidates to compare against the query\n",
        "\n",
        "  # Iterate through the selected classes\n",
        "  for i, selected_class in enumerate(selected_classes):\n",
        "      # Get a random sample of an image from the class\n",
        "      temp_image = train_dataset.data[selected_class][0] #TAKES THE FIRST EXAMPLE OF THE CLASS\n",
        "      temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n",
        "      temp_image *= 255\n",
        "      temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n",
        "      comparisons.append(temp_image)\n",
        "\n",
        "\n",
        "################ VISUALIZATIONS ######################\n",
        "      #display the class\n",
        "      axarr[i // 5, i % 5].set_title(\"Class : \" + selected_class)\n",
        "      axarr[i // 5, i % 5].imshow(temp_image, cmap=\"gray\")\n",
        "      axarr[i // 5, i % 5].xaxis.set_visible(True)\n",
        "      axarr[i // 5, i % 5].yaxis.set_visible(True)\n",
        "  plt.show()\n",
        "################ VISUALIZATIONS ######################\n",
        "\n",
        "\n",
        "  return query,query_class,comparisons,selected_classes\n",
        "\n",
        "\n",
        "\n",
        "#Scale and shift code\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def scale_shift(xy_coords): #shifts all data to center and scales by min max normalization\n",
        "  scaler = MinMaxScaler(feature_range=(0, 28),clip=True)\n",
        "  xy = scaler.fit_transform(xy_coords)\n",
        "  centroid = np.mean(xy,axis = 0)\n",
        "  centroid_x = centroid[0]\n",
        "  centroid_y = centroid[1]\n",
        "  center_x = 14\n",
        "  center_y = 14\n",
        "  x_shift = center_x - centroid_x\n",
        "  y_shift = center_x - centroid_y\n",
        "  centered_x = list(i[0] for i in xy) + x_shift\n",
        "  centered_y = list(i[1] for i in xy) + y_shift\n",
        "  centered_xy = [[i, j] for i, j in zip(centered_x, centered_y)]\n",
        "\n",
        "  return np.array(centered_xy)\n",
        "\n",
        "\n",
        "def pix2cart(image):\n",
        "  xy = np.stack(np.where(image<0.5)).transpose()\n",
        "  xy = xy[:,:2]\n",
        "  xy = list(set([tuple(i) for i in xy]))\n",
        "  xy = [list(i) for i in xy]\n",
        "  return np.array(xy)\n",
        "\n",
        "\n",
        "def synt_char(image, n_examples,density): #returns n different examples of a single character\n",
        "  synt_data = []\n",
        "  range_n = [6,7,8,9,10]\n",
        "  probabilities = [.05,.05,.2,.25,.45]\n",
        "  for i in range(n_examples):\n",
        "    current_n = np.random.choice(range_n, p = probabilities)\n",
        "    a1 = pix2cart(image)\n",
        "    gmm = GaussianMixture(n_components=current_n).fit(a1)\n",
        "    a2,labels = gmm.sample(density) #density\n",
        "    a3 = np.array(a2)\n",
        "    synt_data.append(a3)\n",
        "  return synt_data,labels #xy coords\n",
        "\n",
        "def synt_alphabet(alphabet,n_examples,density):\n",
        "  synthetic_characters = []\n",
        "  labelz = []\n",
        "  for char in tqdm(alphabet):\n",
        "    sc,labels = synt_char(char,n_examples,density)\n",
        "    synthetic_characters.extend(sc)\n",
        "    labelz.append(labels)\n",
        "  return synthetic_characters,labelz\n",
        "\n",
        "\n",
        "def oneshot_within(n,alphabet,visual = False): # n-way different characters from single alphabet\n",
        "\n",
        "  if visual == True:\n",
        "############### VISUALIZATIONS ######################\n",
        "    if n == 5:\n",
        "      _, axarr = plt.subplots(nrows=1, ncols=5, figsize=(10, 3))\n",
        "\n",
        "    elif n == 10:\n",
        "      _, axarr = plt.subplots(nrows=2, ncols=5, figsize=(10, 6))\n",
        "\n",
        "    else:\n",
        "      _, axarr = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
        "############### VISUALIZATIONS ######################\n",
        "\n",
        "  images, labels = n_chars_alphabet(n,alphabet)\n",
        "  selected_classes = labels\n",
        "  query_class = random.sample(selected_classes,1)\n",
        "  query = train_dataset.data[query_class[0]][1]\n",
        "  query = np.stack((query[:, :, 0],) * 3, axis=2)\n",
        "  query *= 255\n",
        "  query = np.clip(query, 0, 255).astype(\"uint8\")\n",
        "  comparisons = images #the other example candidates to compare against the query\n",
        "\n",
        "############### VISUALIZATIONS ######################\n",
        "  if visual == True:\n",
        "    for i in range(len(selected_classes)):\n",
        "      if n == 5:\n",
        "        axarr[i].set_title(\"Class : \" + selected_classes[i])\n",
        "        axarr[i].imshow(images[i], cmap=\"gray\")\n",
        "        axarr[i].xaxis.set_visible(True)\n",
        "        axarr[i].yaxis.set_visible(True)\n",
        "      else:\n",
        "        axarr[i // 5, i % 5].set_title(\"Class : \" + selected_classes[i])\n",
        "        axarr[i // 5, i % 5].imshow(images[i], cmap=\"gray\")\n",
        "        axarr[i // 5, i % 5].xaxis.set_visible(True)\n",
        "        axarr[i // 5, i % 5].yaxis.set_visible(True)\n",
        "\n",
        "    plt.show()\n",
        "############### VISUALIZATIONS ######################\n",
        "\n",
        "\n",
        "  return query,query_class,comparisons, selected_classes\n",
        "\n",
        "\n",
        "#Extracts n different characters from the same alphabet\n",
        "def n_chars_alphabet(n,alphabet):\n",
        "  images = []\n",
        "  labels = sample(alphabet,n)\n",
        "  for i in range(n):\n",
        "    image = train_dataset.data[str(labels[i])][0] #takes first example\n",
        "    image = np.stack((image[:, :, 0],) * 3, axis=2)\n",
        "    image *= 255\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
        "    images.append(image)\n",
        "\n",
        "  return images, [str(i) for i in labels]\n",
        "\n",
        "\n",
        "#Extracts n different characters from the same alphabet\n",
        "def n_chars_alphabet(n,alphabet):\n",
        "  images = []\n",
        "  labels = sample(alphabet,n)\n",
        "  for i in range(n):\n",
        "    image = train_dataset.data[str(labels[i])][0] #takes first example\n",
        "    image = np.stack((image[:, :, 0],) * 3, axis=2)\n",
        "    image *= 255\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
        "    images.append(image)\n",
        "\n",
        "  return images, [str(i) for i in labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aSPdj_YDe1xB"
      },
      "outputs": [],
      "source": [
        "numgen = 100\n",
        "\n",
        "query,query_class,comparisons,selected_classes = oneshot()\n",
        "data,labels = synt_alphabet(comparisons,numgen,300)\n",
        "compared_characters = [cart2pix(k) for k in data]\n",
        "scaled_compared_characters = [scale_shift(i) for i in compared_characters]\n",
        "scaled_char = [cart2pix(k) for k in scaled_compared_characters]\n",
        "sa_normalized = np.reshape(scaled_char,(len(comparisons)*numgen,28,28,1))\n",
        "sa = np.reshape(compared_characters,(len(comparisons)*numgen,28,28,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE Stuff"
      ],
      "metadata": {
        "id": "kpjG7h5WBeKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hqsCKplWbQLX"
      },
      "outputs": [],
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CwWT9-QhbQLX"
      },
      "outputs": [],
      "source": [
        "#Encoder\n",
        "\n",
        "latent_dim = 2\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3eNCjPuQbQLX"
      },
      "outputs": [],
      "source": [
        "#Decoder\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E1aMjByPbQLY"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Synthetic Data for VAE"
      ],
      "metadata": {
        "id": "YbrTIAuABxup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n4CffS8BhpWW"
      },
      "outputs": [],
      "source": [
        "num_examples = 100\n",
        "\n",
        "def preprocess_images(images):\n",
        "  return images.astype('float32')\n",
        "\n",
        "train_images = preprocess_images(sa)\n",
        "train_labels = []\n",
        "for i in range(len(selected_classes)):\n",
        "  train_labels.extend([i]*numgen)\n",
        "\n",
        "train_size = len(comparisons) * num_examples\n",
        "batch_size = 32\n",
        "test_size = len(comparisons) * num_examples\n",
        "\n",
        "s_train_images = tf.random.shuffle(train_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train VAE with Synthetic Data"
      ],
      "metadata": {
        "id": "22GLPeyLcP5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TB1WByG2bQLY"
      },
      "outputs": [],
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n",
        "vae.fit(s_train_images, epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display how the latent space clusters different classes"
      ],
      "metadata": {
        "id": "5smbOYrXDhcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cU3whm0fE0Kp"
      },
      "outputs": [],
      "source": [
        "def plot_label_clusters(vae, data, labels):\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, z_log_var, z = vae.encoder.predict(data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_log_var[:, 0], z_log_var[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"zvar[0]\")\n",
        "    plt.ylabel(\"zvar[1]\")\n",
        "    plt.show()\n",
        "\n",
        "    xmin = min(z_mean[:, 0])\n",
        "    xmax = max(z_mean[:, 0])\n",
        "    ymin = min(z_mean[:, 1])\n",
        "    ymax = max(z_mean[:, 1])\n",
        "\n",
        "    return xmin, xmax, ymin, ymax\n",
        "\n",
        "\n",
        "xmin, xmax, ymin, ymax = plot_label_clusters(vae, train_images,train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Omniglot Latent Space"
      ],
      "metadata": {
        "id": "iI0lvOW-D1h4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-WuZ-Az-bQLY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_latent_space(vae, n=7, figsize=15):\n",
        "    # display an n*n 2D manifold of digits\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(xmin, xmax, n)\n",
        "    grid_y = np.linspace(ymin, ymax, n)[::-1]\n",
        "\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[\n",
        "                i * digit_size : (i + 1) * digit_size,\n",
        "                j * digit_size : (j + 1) * digit_size,\n",
        "            ] = digit\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(28 - figure, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent_space(vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample A Single Example \\\\\n",
        "\n",
        "Visualize Different Thresholds"
      ],
      "metadata": {
        "id": "G_Bh4pXnD6NS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw3E3uIDcr4J"
      },
      "outputs": [],
      "source": [
        "def single(x,y,vae,figsize=15):\n",
        "    # display an n*n 2D manifold of digits\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size, digit_size))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    z_sample = np.array([[x, y]])\n",
        "    x_decoded = vae.decoder.predict(z_sample)\n",
        "    digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "    plt.imshow(digit,cmap=\"Greys_r\")\n",
        "    return digit\n",
        "\n",
        "d = single(1.3,1.4,vae,figsize=15) #example z in (1.3,1.4) of latent space\n",
        "min_value = np.min(d)\n",
        "max_value = np.max(d)\n",
        "normalized_image = (d - min_value) / (max_value - min_value)\n",
        "scaled_image = (normalized_image * 27).astype(int)\n",
        "threshold = range(0,15)\n",
        "# binary_image = (scaled_image > threshold).astype(int)\n",
        "# plt.imshow(skeletonize(binary_image))\n",
        "\n",
        "# for i in threshold:\n",
        "#   plt.subplots()\n",
        "#   binary_image = (scaled_image > i).astype(int)\n",
        "#   plt.imshow(28 - skeletonize(binary_image),cmap = \"gray\")\n",
        "fig, axes = plt.subplots(3, 5, figsize=(10, 5))\n",
        "\n",
        "# Iterate over thresholds and plot the skeletonized binary images\n",
        "for i, threshold in enumerate(threshold):\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "\n",
        "    # Calculate the binary image with the current threshold\n",
        "    binary_image = (scaled_image > threshold).astype(int)\n",
        "\n",
        "    # Skeletonize the binary image\n",
        "    skeletonized_image = skeletonize(binary_image)\n",
        "\n",
        "    # Display the skeletonized image in the current subplot\n",
        "    axes[row, col].imshow(28 - skeletonized_image, cmap=\"gray\")\n",
        "    axes[row, col].set_title(f'Threshold: {threshold}')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: New Generated Character Given Alphabet"
      ],
      "metadata": {
        "id": "IV5Io99iEUBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGfe0ty9dSvo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.morphology import skeletonize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def save_real(alphabet, selected_classes):\n",
        "    not_in = [i for i in alphabet if i not in selected_classes]\n",
        "    print(not_in)\n",
        "    for i in not_in:\n",
        "        image = train_dataset.data[i][0]\n",
        "        image = cv2.resize(image, (28, 28))\n",
        "\n",
        "        inverted_image_np = 255 - image  # Invert the image\n",
        "        min_value = np.min(inverted_image_np)\n",
        "        max_value = np.max(inverted_image_np)\n",
        "        normalized_image = (inverted_image_np - min_value) / (max_value - min_value)\n",
        "        scaled_image = (normalized_image * 27).astype(int)\n",
        "        binary_image = (scaled_image > 1).astype(int)\n",
        "\n",
        "        # Create a new figure and axis\n",
        "        #fig, ax = plt.subplots()\n",
        "        plt.imshow(28 - skeletonize(binary_image),cmap='gray')\n",
        "\n",
        "        # Save the figure with a filename\n",
        "        filename = f'real_{i}.png'\n",
        "        plt.imsave(filename, 28 - skeletonize(binary_image),cmap='gray')\n",
        "\n",
        "\n",
        "save_real(all_alphabets[26],selected_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <!-- We want to spatially represent the latent space:\n",
        "# 1. Run a GMM with the specfies number of classes (20) in z\n",
        "# 2. See the probability distribution of where the query falls\n",
        "# 3. Use the similarity metric to weigh the proabilities.\n",
        "# - A (0.3) , B (0.5) , C (0.2)\n",
        "# - Sim(Q,A) = 200, Sim(Q,B) = 150, Sim(Q,C) = 20\n",
        "# - Final score for A = 200*0.3, B = 0.5*150, C = 20*.2\n",
        "# - Winner is B\n",
        "# (We might have to transform the similarity scores into a probability to keep it consistent and meaningful) -->\n"
      ],
      "metadata": {
        "id": "E3nv1wc3cC6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}